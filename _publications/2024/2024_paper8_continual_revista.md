---
title:          "Continual learning, deep reinforcement learning, and microcircuits: a novel method for clever game playing"
date:           2024-04-01 00:01:00 +0800
selected:       true
pub:            "Multimedia Tools and Applications"
# pub_pre:        "Submitted to "
# pub_post:       'Under review.'
pub_last:       ' <span class="badge badge-pill badge-publication badge-success">Open Access</span>'
pub_date:       "2024"
semantic_scholar_id: b6607f0cf3847176f296a9dc0602707dfcb7f555  # use this to retrieve citation count
abstract: >-
  Contemporary neural networks frequently encounter the challenge of catastrophic forgetting, wherein newly acquired learning can overwrite and erase previously learned information. The paradigm of continual learning offers a promising solution by enabling intelligent systems to retain and build upon their acquired knowledge over time. This paper introduces a novel approach within the continual learning framework, employing deep reinforcement learning agents that process unprocessed pixel data and interact with microcircuit-like components. These agents autonomously advance through a series of learning stages, culminating in the development of a sophisticated neural network system optimized for predictive performance in the game of tic-tac-toe. Structured to operate in sequential order, each agent is tasked with achieving forward-looking objectives based on Bellmanâ€™s principles of reinforcement learning. Knowledge retention is facilitated through the integration of specific microcircuits, which securely store the insights gained by each agent. During the training phase, these microcircuits work in concert, employing high-energy, sparse encoding techniques to enhance learning efficiency and effectiveness. The core contribution of this paper is the establishment of an artificial neural network system capable of accurately predicting tic-tac-toe moves, akin to the observational strategies employed by humans. Our experimental results demonstrate that after approximately 5000 cycles of backpropagation, the system significantly reduced the training loss to , thereby increasing the expected cumulative reward. This advancement in training efficiency translates into superior predictive capabilities, enabling the system to secure consistent victories by anticipating up to four moves ahead.
cover:          /assets/images/covers/cover_continual_chang.png
authors:
  - Oscar Chang
  - Leo Ramos
  - Manuel Eugenio Morocho-Cayamcela
  - Rolando Armas 
  - Luis Zhinin-Vera 

 
links:
  Paper: https://doi.org/10.1007/s11042-024-18925-2
  #Unsplash: https://unsplash.com/photos/sliced-in-half-pineapple--_PLJZmHZzk
---
